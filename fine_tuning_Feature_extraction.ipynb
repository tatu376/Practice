{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fine tuning_Feature extraction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1p7h6up0fX7",
        "colab_type": "code",
        "outputId": "5cdf6f5a-3630-4369-c8f1-9aa185ee051a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        }
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "from imutils import paths\n",
        "from keras.applications import VGG16\n",
        "from keras.applications import imagenet_utils\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.preprocessing.image import load_img\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.optimizers import SGD\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.applications import VGG16, ResNet50, InceptionResNetV2\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "from keras.layers.core import Dense\n",
        "from keras.layers.core import Dropout\n",
        "from keras.layers.core import Flatten\n",
        "import numpy as np\n",
        "import random\n",
        "random\n",
        "import zipfile\n",
        "import os"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c68NFl88fda",
        "colab_type": "text"
      },
      "source": [
        "> # **1. Pre-processing Image** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vjF3F7H5FK-",
        "colab_type": "text"
      },
      "source": [
        "import file from /tmp"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKhI_tSe0vz-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# #un-zip the dataset file\n",
        "# local_zip = '/tmp/dataset.zip'\n",
        "# zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "# zip_ref.extractall('/tmp')\n",
        "# zip_ref.close()\n",
        "\n",
        "# #return the shuffled image path \n",
        "# image_path = list(paths.list_images('/tmp/dataset/'))\n",
        "# random.shuffle(image_path)\n",
        "\n",
        "# #extract the lable, because the directory format is dataset/flower_name/image_name \n",
        "# # so the name of the flower will be extracted as follow:\n",
        "\n",
        "# labels = [p.split(os.path.sep)[-2] for p in image_path]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAeAvRXc5KQa",
        "colab_type": "text"
      },
      "source": [
        "**Import file from Drive upload**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nN5HY5B36xLL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Mount drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pk9F568dk_pS",
        "colab_type": "code",
        "outputId": "82e74b40-b3c1-4318-f84c-1b7fcb1fd225",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bv0oxQq22zXf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# #un-zip the dataset file\n",
        "# local_zip = '/content/drive/My Drive/CV References/dataset.zip'\n",
        "# zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "# zip_ref.extractall('/content/drive/My Drive/CV References/')\n",
        "# zip_ref.close()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXkfibNQ267z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#return the shuffled image path \n",
        "image_path = list(paths.list_images('/content/drive/My Drive/Machine Learning/References/dataset')) \n",
        "random.shuffle(image_path)\n",
        "\n",
        "#extract the lable, because the directory format is dataset/flower_name/image_name \n",
        "# so the name of the flower will be extracted as follow:\n",
        "\n",
        "labels = [p.split(os.path.sep)[-2] for p in image_path]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yU8wFG9R4tCY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Encode\n",
        "le = LabelEncoder()\n",
        "labels = le.fit_transform(labels)\n",
        "\n",
        "# One-hot encoding\n",
        "lb = LabelBinarizer()\n",
        "labels = lb.fit_transform(labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyuYvGQDKzNr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from glob import glob\n",
        "# import re\n",
        "\n",
        "# path = glob('/content/drive/My Drive/Machine Learning/CV References/dataset/*.jpg')\n",
        "# list_image=[]\n",
        "# for img_path in path:\n",
        "#   # img_path = re.findall(r\"\\w.jpg\", img_path)\n",
        "#   img = cv2.imread(str(img_path))\n",
        "#   img=np.expand_dims(img,0)\n",
        "#   list_image.append(img)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGF9YkYjOBRv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6f82e894-fda6-4ac3-f6f8-c9d905daf2f4"
      },
      "source": [
        "path"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gn6whlKD44kz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load image and resize for VGG model (224,224)\n",
        "list_image = []\n",
        "for (j, imagePath) in enumerate(image_path):\n",
        "    image = load_img(imagePath, target_size=(224, 224))\n",
        "    image = img_to_array(image)\n",
        "    \n",
        "    image = np.expand_dims(image, 0)\n",
        "    image = imagenet_utils.preprocess_input(image)\n",
        "    \n",
        "    list_image.append(image)\n",
        "    \n",
        "list_image = np.vstack(list_image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeHIJzDEGtpP",
        "colab_type": "text"
      },
      "source": [
        "> # **2.Build model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjtf5WGu7nc4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load model VGG16. soft-max not included\n",
        "baseModel = VGG16(weights='imagenet', include_top=False, input_tensor=Input(shape=(224, 224, 3)))\n",
        "\n",
        "# Add layers\n",
        "# output of VGG16 (last layer but not soft-max layer)\n",
        "x = baseModel.output\n",
        "# Flatten \n",
        "x = Flatten(name='flatten')(x)\n",
        "# FC\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "# Output layer with softmax activation\n",
        "x = Dense(512, activation='relu')(x)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "x = Dropout(0.25)(x)\n",
        "x = Dense(17, activation='softmax')(x)\n",
        "\n",
        "\n",
        "model = model = Model(inputs=baseModel.input, outputs=x)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5pkTnvfF7tC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(list_image, labels, test_size=0.2, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRygjJJpG9ik",
        "colab_type": "text"
      },
      "source": [
        "> # **3. Data Augmentation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "963Az0b8F_iI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Data Augmentation\n",
        "# training data\n",
        "aug_train = ImageDataGenerator(rescale=1./255, rotation_range=30, width_shift_range=0.1, height_shift_range=0.1, shear_range=0.2, \n",
        "                         zoom_range=0.2, horizontal_flip=True, fill_mode='nearest')\n",
        "# test data\n",
        "aug_test= ImageDataGenerator(rescale=1./255)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pv6qoSnO8yQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoQB7mZeNQH7",
        "colab_type": "text"
      },
      "source": [
        "> # **4. Callback, Checkpoint and Early stopping**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03UxL0wbNZH7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "checkpoint = ModelCheckpoint(\"/content/drive/My Drive/Machine Learning/CV References/dataset/Checkpoint.h5\",\n",
        "                             monitor=\"val_loss\",\n",
        "                             mode=\"min\",\n",
        "                             save_best_only = True,\n",
        "                             verbose=1)\n",
        "\n",
        "earlystop = EarlyStopping(monitor = 'val_loss', # value being monitored for improvement\n",
        "                          min_delta = 0, #Abs value and is the min change required before we stop\n",
        "                          patience = 3, #Number of epochs we wait before stopping \n",
        "                          verbose = 1,\n",
        "                          restore_best_weights = True) #keeps the best weigths once stopped\n",
        "                          \n",
        "reduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.2, patience = 3, verbose = 1, min_delta = 0.0001)\n",
        "\n",
        "callbacks = [earlystop, checkpoint, reduce_lr]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMoG_feKHEJG",
        "colab_type": "text"
      },
      "source": [
        "> # **5.Training with all frozen layers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qdrb0-xLGIrz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # freeze VGG16 model\n",
        "# for layer in baseModel.layers:\n",
        "#     layer.trainable = False\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEEDBHABIP_x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 943
        },
        "outputId": "8306c336-d596-480b-ebad-d20587698660"
      },
      "source": [
        "\n",
        "# model.compile('adam', 'categorical_crossentropy', ['accuracy'])\n",
        "# numOfEpoch = 10\n",
        "# H = model.fit_generator(aug_train.flow(X_train, y_train, batch_size=32), \n",
        "#                         steps_per_epoch=len(X_train)//32,\n",
        "#                         validation_data=(aug_test.flow(X_test, y_test, batch_size=32)),\n",
        "#                         validation_steps=len(X_test)//32,\n",
        "#                         epochs=numOfEpoch,\n",
        "#                         callbacks=callbacks)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/10\n",
            "33/33 [==============================] - 14s 419ms/step - loss: 3.1947 - acc: 0.1879 - val_loss: 1.7332 - val_acc: 0.5039\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.73323, saving model to /content/drive/My Drive/Machine Learning/CV References/dataset/Checkpoint.h5\n",
            "Epoch 2/10\n",
            "33/33 [==============================] - 11s 345ms/step - loss: 1.5080 - acc: 0.5229 - val_loss: 0.9239 - val_acc: 0.7333\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.73323 to 0.92387, saving model to /content/drive/My Drive/Machine Learning/CV References/dataset/Checkpoint.h5\n",
            "Epoch 3/10\n",
            "33/33 [==============================] - 11s 332ms/step - loss: 0.9730 - acc: 0.6898 - val_loss: 0.6648 - val_acc: 0.7917\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.92387 to 0.66482, saving model to /content/drive/My Drive/Machine Learning/CV References/dataset/Checkpoint.h5\n",
            "Epoch 4/10\n",
            "33/33 [==============================] - 11s 340ms/step - loss: 0.7966 - acc: 0.7417 - val_loss: 0.5250 - val_acc: 0.8375\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.66482 to 0.52497, saving model to /content/drive/My Drive/Machine Learning/CV References/dataset/Checkpoint.h5\n",
            "Epoch 5/10\n",
            "33/33 [==============================] - 11s 334ms/step - loss: 0.5301 - acc: 0.8310 - val_loss: 0.5236 - val_acc: 0.8542\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.52497 to 0.52360, saving model to /content/drive/My Drive/Machine Learning/CV References/dataset/Checkpoint.h5\n",
            "Epoch 6/10\n",
            "33/33 [==============================] - 11s 334ms/step - loss: 0.4770 - acc: 0.8511 - val_loss: 0.5074 - val_acc: 0.8333\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.52360 to 0.50742, saving model to /content/drive/My Drive/Machine Learning/CV References/dataset/Checkpoint.h5\n",
            "Epoch 7/10\n",
            "33/33 [==============================] - 10s 318ms/step - loss: 0.4047 - acc: 0.8729 - val_loss: 0.3560 - val_acc: 0.8917\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.50742 to 0.35601, saving model to /content/drive/My Drive/Machine Learning/CV References/dataset/Checkpoint.h5\n",
            "Epoch 8/10\n",
            "33/33 [==============================] - 11s 330ms/step - loss: 0.3650 - acc: 0.8710 - val_loss: 0.4278 - val_acc: 0.8333\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.35601\n",
            "Epoch 9/10\n",
            "33/33 [==============================] - 11s 344ms/step - loss: 0.4304 - acc: 0.8588 - val_loss: 0.5384 - val_acc: 0.8458\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.35601\n",
            "Epoch 10/10\n",
            "33/33 [==============================] - 11s 335ms/step - loss: 0.2950 - acc: 0.9053 - val_loss: 0.4378 - val_acc: 0.8594\n",
            "Restoring model weights from the end of the best epoch\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.35601\n",
            "\n",
            "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "Epoch 00010: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJIX1eAKPVet",
        "colab_type": "text"
      },
      "source": [
        "> # **6. Training with unfrozen layers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlvpUYhSPcro",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# unfreeze last layer of VGG16 model\n",
        "for layer in baseModel.layers[220:]:\n",
        "    layer.trainable = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23oLYrOKPrWI",
        "colab_type": "code",
        "outputId": "02647a12-6701-44e2-9187-d2a49f94bcad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        }
      },
      "source": [
        "model.compile('adam', 'categorical_crossentropy', ['accuracy'])\n",
        "numOfEpoch = 10\n",
        "H = model.fit_generator(aug_train.flow(X_train, y_train, batch_size=4), \n",
        "                        steps_per_epoch=len(X_train)//4,\n",
        "                        validation_data=(aug_test.flow(X_test, y_test, batch_size=4)),\n",
        "                        validation_steps=len(X_test)//4,\n",
        "                        epochs=numOfEpoch,\n",
        "                        callbacks=callbacks)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "271/271 [==============================] - 20s 75ms/step - loss: 2.8356 - acc: 0.0452 - val_loss: 2.8358 - val_acc: 0.0404\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 2.83347\n",
            "Epoch 2/10\n",
            "271/271 [==============================] - 18s 65ms/step - loss: 2.8339 - acc: 0.0581 - val_loss: 2.8375 - val_acc: 0.0404\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 2.83347\n",
            "Epoch 3/10\n",
            "271/271 [==============================] - 18s 67ms/step - loss: 2.8333 - acc: 0.0581 - val_loss: 2.8389 - val_acc: 0.0331\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 2.83347\n",
            "Epoch 4/10\n",
            "271/271 [==============================] - 18s 66ms/step - loss: 2.8334 - acc: 0.0590 - val_loss: 2.8405 - val_acc: 0.0331\n",
            "Restoring model weights from the end of the best epoch\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 2.83347\n",
            "\n",
            "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "Epoch 00004: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RQiZG18PtXq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "6768945e-4dc4-4a64-96d3-0a18aa43dfcb"
      },
      "source": [
        "from keras.models import load_model\n",
        "smodel = load_model('/content/drive/My Drive/Machine Learning/CV References/dataset/Checkpoint.h5')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4271: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iglQqlCpYSh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}